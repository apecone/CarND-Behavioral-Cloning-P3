{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import Sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Lambda\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.advanced_activations import ELU\n",
    "from keras.regularizers import l2\n",
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import csv\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "import multiprocessing\n",
    "\n",
    "# Generator class inspired by tutorial at \n",
    "# https://stanford.edu/~shervine/blog/keras-how-to-generate-data-on-the-fly\n",
    "class DataGenerator(Sequence):\n",
    "    'Generates data for Keras'\n",
    "    def __init__(self, data=None, data_path='/opt/carnd_p3/data/', batch_size=32, shuffle=True):\n",
    "        'Initialization'\n",
    "        #self.data = data\n",
    "        self.data_path = data_path\n",
    "        self.batch_size = batch_size\n",
    "        self.shuffle = shuffle\n",
    "        self.lines = data\n",
    "        #self.__get_lines(os.path.join(self.data_path,'driving_log.csv'))\n",
    "        self.numImages = len(self.lines) * 6\n",
    "        self.on_epoch_end()\n",
    "\n",
    "    def __len__(self):\n",
    "        'Denotes the number of batches per epoch'\n",
    "        return self.numImages // self.batch_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        'Generate one batch of data'\n",
    "        # Generate indexes of the batch\n",
    "        image_IDs_temp = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n",
    "\n",
    "        # Generate data\n",
    "        X, y = self.__data_generation(image_IDs_temp)\n",
    "\n",
    "        return X, y\n",
    "    \n",
    "    def on_epoch_end(self):\n",
    "        'Updates indexes after each epoch'\n",
    "        self.indexes = np.arange(self.numImages)\n",
    "        if self.shuffle == True:\n",
    "            np.random.shuffle(self.indexes)\n",
    "            \n",
    "    def __data_generation(self, image_IDs_temp):\n",
    "        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n",
    "        # Initialization\n",
    "        images = []\n",
    "        measurements = []\n",
    "        num_lines = len(self.lines)\n",
    "        for imgID in image_IDs_temp:\n",
    "            line_offset = imgID % num_lines\n",
    "            image_offset = imgID // num_lines\n",
    "            \n",
    "            # Get the line\n",
    "            line = self.lines[line_offset]\n",
    "            \n",
    "            # Get the image\n",
    "            source_path = line[image_offset % 3]\n",
    "            filename = source_path.split('/')[-1]\n",
    "            current_path = os.path.join(*[self.data_path,'IMG', filename])\n",
    "            image = cv2.imread(current_path)\n",
    "            image = image[:,:,::-1]\n",
    "            \n",
    "            # Get the measurement\n",
    "            measurement = float(line[3])\n",
    "            if image_offset == 1:\n",
    "                measurement += 0.2\n",
    "            elif image_offset == 2:\n",
    "                measurement -= 0.2\n",
    "                \n",
    "            # If offset is 3, 4, 5; flip image and measurement\n",
    "            if image_offset in range(3,6):\n",
    "                image = cv2.flip(image, 1)\n",
    "                measurement *= -1.0\n",
    "                \n",
    "            # Crop image\n",
    "            image = self.__crop_image(image)\n",
    "            \n",
    "            # Append image and measurement to batch\n",
    "            images.append(image)\n",
    "            measurements.append(measurement)\n",
    "\n",
    "        return np.array(images), np.array(measurements)\n",
    "    \n",
    "    def __crop_image(self, image):\n",
    "        blur = cv2.GaussianBlur(image, (3,3), 0)\n",
    "        crop = blur[50:150,:,:]\n",
    "        resize = cv2.resize(crop, (200, 66), interpolation = cv2.INTER_AREA)\n",
    "        \n",
    "        return resize\n",
    "    \n",
    "    '''\n",
    "    def __get_lines(self, driving_log):\n",
    "        lines = []\n",
    "        with open(driving_log) as csvfile:\n",
    "            reader = csv.reader(csvfile)\n",
    "            for line in reader:\n",
    "                lines.append(line)\n",
    "                \n",
    "        return lines[1:]\n",
    "    '''\n",
    "\n",
    "\n",
    "def get_model(input_shape=(66,200,3)):\n",
    "    \n",
    "    _model = Sequential()\n",
    "    \n",
    "    _model.add(Lambda(lambda x: x / 127.5 - 1.0, input_shape=input_shape))\n",
    "    \n",
    "    _model.add(Conv2D(filters=24, kernel_size=(5,5), strides=(2,2), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    _model.add(Conv2D(filters=36, kernel_size=(5,5), strides=(2,2), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    _model.add(Conv2D(filters=48, kernel_size=(5,5), strides=(2,2), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    \n",
    "    _model.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    _model.add(Conv2D(filters=64, kernel_size=(3,3), padding='valid', kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    \n",
    "    _model.add(Flatten())\n",
    "    \n",
    "    _model.add(Dense(100, kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    _model.add(Dense(50, kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    _model.add(Dense(10, kernel_regularizer=l2(0.001)))\n",
    "    _model.add(ELU())\n",
    "    \n",
    "    _model.add(Dense(1))\n",
    "    \n",
    "    \n",
    "    #_model.fit(X_train, y_train, validation_split=0.2, shuffle=True, epochs=7)\n",
    "    \n",
    "    return _model\n",
    "\n",
    "\n",
    "def get_lines(driving_log):\n",
    "    lines = []\n",
    "    with open(driving_log) as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        for line in reader:\n",
    "            lines.append(line)\n",
    "\n",
    "    return lines[1:]\n",
    "\n",
    "\n",
    "def train_test_split(data, ratio=0.8):\n",
    "    train_indices = np.random.choice(len(data), int((len(data)*ratio)//1), replace=False)\n",
    "    test_indices = np.setdiff1d(np.arange(len(data)), train_indices)\n",
    "    \n",
    "    train = []\n",
    "    for i in train_indices:\n",
    "        train.append(data[i])\n",
    "        \n",
    "    test = []\n",
    "    for i in test_indices:\n",
    "        test.append(data[i])\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "\n",
    "def main():\n",
    "    # Select batch size\n",
    "    batch_size = 32\n",
    "    \n",
    "    # Load in data\n",
    "    data = get_lines('data/driving_log.csv')\n",
    "    training_data, remaining_data = train_test_split(data, 0.8)\n",
    "    validation_data, testing_data = train_test_split(remaining_data, 0.2)\n",
    "    \n",
    "    # Create generator\n",
    "    training_generator = DataGenerator(data=training_data, data_path='data', batch_size=batch_size)\n",
    "    validation_generator = DataGenerator(data=validation_data, data_path='data', batch_size=batch_size)\n",
    "    testing_generator = DataGenerator(data=testing_data, data_path='data', batch_size=batch_size)\n",
    "    \n",
    "    \n",
    "    # Load model\n",
    "    model = get_model((66,200,3))\n",
    "    \n",
    "    # Create model checkpoint\n",
    "    filepath=\"model.h5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "    callbacks_list = [checkpoint]\n",
    "    \n",
    "    \n",
    "    # Compile model\n",
    "    model.compile(loss='mse', optimizer=Adam(lr=0.001))\n",
    "    \n",
    "    # Train model\n",
    "    model.fit_generator(generator=training_generator, \n",
    "                        steps_per_epoch=len(training_data)//batch_size,\n",
    "                        epochs=100,\n",
    "                        validation_data=validation_generator,\n",
    "                        validation_steps=len(validation_data)//batch_size,\n",
    "                        use_multiprocessing=True, \n",
    "                        workers=multiprocessing.cpu_count()-1, \n",
    "                        callbacks=callbacks_list)\n",
    "    \n",
    "#if __name__ == \"__main__\":\n",
    "    #main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
